
# Predict Student's Dropout And Academic Success
### Name: Sai Krishna Vamsi
### Reg No: 12001567


Dataset: <a href="https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention">https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention</a>
import numpy as np
import pandas as pd
import scipy
import matplotlib.pyplot as plt
from pylab import rcParams
import urllib
import sklearn
from sklearn.preprocessing import OneHotEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn import neighbors
from sklearn import tree, model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,MaxAbsScaler
from sklearn import utils
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import GridSearchCV
import scipy.stats as st
students = pd.read_csv('dataset.csv')

#In case I need to drop some columns that are skewing the results I create another df
students_reduced = students#.drop(columns = "Marital status")
#col is the number of columns in active df
col = 34
students.head()
#We need 1 or 0 in the target column so I create an array and change dropout to 1 and graduate to 0 and replace it.
target=students_reduced.iloc[:,col]
target=np.array(target)                
for i in range(len(target)):
    if target[i] == "Dropout": target[i] = '1'
    if target[i] == "Graduate": target[i] = '0' 
students_reduced["Target"] = target
#In the target column there are also students that are enrolled and we dont need those in analysis so we drop them, as well
#as random numbers that got created along the way
students_reduced = students_reduced[students_reduced.Target != 'Enrolled'] 
students_reduced = students_reduced[(students_reduced.Target != '1')  | (students_reduced.Target != '0')]
#Next we set X as all the columns beside the target and y as the target
X = students_reduced.iloc[:, :-1].values
y = students_reduced.iloc[:, col].values
#We use the StandardScaler on the X data
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)

#We use the train_test_split function
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
#We use KNN function as the classifier
classifier = KNeighborsClassifier(n_neighbors=8, weights='distance')
#Then we fit X_train and y_train
classifier.fit(X_train, y_train)
#To find the most effective parameter combination of KNeighborsClassifier we use grid_search where we set the Map first and 
#then we fit it on X and y axes
grid_search_params = {
    'n_neighbors' : [1,2,3,4,8,9,14,19,20,25],
    'weights' : ['uniform','distance'],
    'algorithm' : ['auto','ball_tree','kd_tree','brute']
    }
gs = GridSearchCV(classifier, grid_search_params, cv = 10, scoring="accuracy")
gs.fit(X,y)
#Last but not least we print out the best parameters and the best score which in this case is :
#Best parameters: KNeighborsClassifier(n_neighbors=8, weights='distance')
#Score : 0.8608815426997245
print("Best parameters:",gs.best_estimator_)
print("Best score :",gs.best_score_)
